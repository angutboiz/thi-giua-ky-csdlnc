from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# ğŸ”¥ 1. Khá»Ÿi táº¡o SparkSession
spark = SparkSession.builder.appName("CustomerClustering").getOrCreate()

# ğŸŸ¢ 2. Äá»c dá»¯ liá»‡u CSV
student_id = "172100123"
df = spark.read.csv(f"transactions_{student_id}.csv", header=True, inferSchema=True)

# ğŸŸ£ 3. PhÃ¢n cá»¥m khÃ¡ch hÃ ng báº±ng KMeans (Spark ML)
# âœ… TÃ­nh tá»•ng sá»‘ Ä‘Æ¡n hÃ ng & tá»•ng chi tiÃªu theo tá»«ng khÃ¡ch hÃ ng
customer_data = df.groupBy("customer_id").agg(
    {"transaction_id": "count", "price": "sum"}
).withColumnRenamed("count(transaction_id)", "total_orders") \
 .withColumnRenamed("sum(price)", "total_spent")

# âœ… Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ nh dáº¡ng vector cho KMeans
assembler = VectorAssembler(inputCols=["total_orders", "total_spent"], outputCol="features")
customer_features = assembler.transform(customer_data)

# âœ… Ãp dá»¥ng KMeans vá»›i K=3
kmeans = KMeans(k=3, seed=1, featuresCol="features", predictionCol="cluster")
model = kmeans.fit(customer_features)
clustered_customers = model.transform(customer_features)

# ğŸ¨ 4. Trá»±c quan hÃ³a báº±ng Seaborn
import pandas as pd

# âœ… Chuyá»ƒn dá»¯ liá»‡u sang Pandas Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“
clustered_pd = clustered_customers.select("customer_id", "total_orders", "total_spent", "cluster").toPandas()

# âœ… Váº½ scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x="total_orders", y="total_spent", hue="cluster", data=clustered_pd, palette="viridis", s=100)
plt.xlabel("Tá»•ng sá»‘ Ä‘Æ¡n hÃ ng")
plt.ylabel("Tá»•ng chi tiÃªu")
plt.title("PhÃ¢n cá»¥m khÃ¡ch hÃ ng báº±ng KMeans")
plt.legend(title="Cá»¥m")
plt.grid()
plt.show()

print("\nâœ… PhÃ¢n tÃ­ch hoÃ n thÃ nh!")